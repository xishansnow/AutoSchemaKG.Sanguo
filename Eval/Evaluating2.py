import json
import os
import re
from openai import OpenAI

# ================= C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N =================
# 1. File vƒÉn b·∫£n g·ªëc (Markdown)
TEXT_FILE_PATH = r"E:\AutoSchemaKG\data\parsed\AMA_Family_Guide_content.md"

# 2. File k·∫øt qu·∫£ Triples (JSON)
TRIPLES_FILE_PATH = r"E:\AutoSchemaKG\output\Phase2_Response.json"

# C·∫•u h√¨nh LM Studio
LM_STUDIO_URL = "http://localhost:1234/v1"
MODEL_ID = "local-model" 

client = OpenAI(base_url=LM_STUDIO_URL, api_key="lm-studio")

def clean_json_string(text):
    """L√†m s·∫°ch chu·ªói JSON tr·∫£ v·ªÅ t·ª´ LLM"""
    text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^```\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\s*```$', '', text, flags=re.MULTILINE)
    return text.strip()

def load_full_text(file_path):
    """ƒê·ªçc to√†n b·ªô n·ªôi dung file Markdown"""
    print(f"üìñ Loading text from: {file_path}...")
    if not os.path.exists(file_path):
        print("‚ùå Error: Text file not found.")
        return ""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def find_context_window(full_text, head, tail, window_size=1000):
    """
    T√¨m ƒëo·∫°n vƒÉn b·∫£n ch·ª©a c·∫£ Head v√† Tail (ho·∫∑c √≠t nh·∫•t l√† Head).
    Tr·∫£ v·ªÅ ƒëo·∫°n text xung quanh (context window) ƒë·ªÉ l√†m b·∫±ng ch·ª©ng.
    """
    # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ t√¨m ki·∫øm kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
    text_lower = full_text.lower()
    head_lower = head.lower()
    tail_lower = tail.lower()
    
    # ∆Øu ti√™n 1: T√¨m v·ªã tr√≠ m√† c·∫£ Head v√† Tail xu·∫•t hi·ªán g·∫ßn nhau
    # T√¨m v·ªã tr√≠ head
    start_idx = text_lower.find(head_lower)
    
    if start_idx == -1:
        # N·∫øu kh√¥ng th·∫•y Head, th·ª≠ t√¨m Tail (fallback)
        start_idx = text_lower.find(tail_lower)
    
    if start_idx != -1:
        # N·∫øu t√¨m th·∫•y, l·∫•y ƒëo·∫°n text xung quanh v·ªã tr√≠ ƒë√≥
        start_window = max(0, start_idx - window_size // 2)
        end_window = min(len(full_text), start_idx + window_size // 2)
        return full_text[start_window:end_window]
    
    return None

def evaluate_triple_accuracy(evidence_text, triple_str):
    """G·ª≠i b·∫±ng ch·ª©ng v√† triple cho LLM ƒë·ªÉ ch·∫•m ƒëi·ªÉm"""
    prompt = f"""You are an expert Knowledge Graph Evaluator.
Verify if the extracted Triple is supported by the Source Text snippet.

### Source Text Snippet:
"...{evidence_text}..."

### Extracted Triple:
{triple_str}

### Task:
Determine if the triple is correct based **ONLY** on the provided text snippet.
- **TP (True Positive)**: The text explicitly supports this relationship.
- **FP (False Positive)**: The text contradicts this or does not mention this relationship.
- **FN (False Negative)**: (Ignore for single triple verification).

### Output (JSON Only):
{{
  "reasoning": "Brief explanation",
  "result": "TP" or "FP"
}}
"""
    try:
        response = client.chat.completions.create(
            model=MODEL_ID,
            messages=[{"role": "system", "content": "Return valid JSON only."}, 
                      {"role": "user", "content": prompt}],
            temperature=0.0,
            stream=False
        )
        res = json.loads(clean_json_string(response.choices[0].message.content))
        return res.get('result', 'FP')
    except Exception:
        return 'FP' # N·∫øu l·ªói coi nh∆∞ sai

def main():
    print(f"üöÄ STARTING EVALUATION (Search & Verify Mode)")
    print("-" * 60)

    # 1. Load d·ªØ li·ªáu
    full_text = load_full_text(TEXT_FILE_PATH)
    if not full_text: return

    if not os.path.exists(TRIPLES_FILE_PATH):
        print(f"‚ùå Error: Triples file not found at {TRIPLES_FILE_PATH}")
        return

    with open(TRIPLES_FILE_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
        all_triples = data.get('all_triples', [])

    total_tp = 0
    total_fp = 0
    processed_count = 0
    
    # ƒê·ªÉ ti·∫øt ki·ªám th·ªùi gian, ta c√≥ th·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng triple test (v√≠ d·ª• 100 c√°i ƒë·∫ßu)
    # N·∫øu mu·ªën ch·∫°y h·∫øt th√¨ b·ªè d√≤ng [:50]
    test_limit = 2200
    print(f"Found {len(all_triples)} triples. Evaluating first {test_limit} triples...")

    for i, t in enumerate(all_triples[:test_limit]):
        head = t.get('head', '')
        tail = t.get('tail', '')
        relation = t.get('relation', '')
        
        triple_str = f"({head}) --[{relation}]--> ({tail})"
        
        # 2. T√¨m b·∫±ng ch·ª©ng trong vƒÉn b·∫£n g·ªëc
        evidence = find_context_window(full_text, head, tail)
        
        print(f"[{i+1}/{test_limit}] Checking: {triple_str} ... ", end="", flush=True)
        
        if evidence:
            # 3. Nh·ªù LLM ch·∫•m ƒëi·ªÉm
            result = evaluate_triple_accuracy(evidence, triple_str)
            if result == 'TP':
                total_tp += 1
                print("‚úÖ TP")
            else:
                total_fp += 1
                print("‚ùå FP")
        else:
            # N·∫øu kh√¥ng t√¨m th·∫•y Head/Tail trong vƒÉn b·∫£n -> Ch·∫Øc ch·∫Øn l√† hallucination (FP)
            total_fp += 1
            print("‚ö†Ô∏è Not found in text (FP)")
        
        processed_count += 1

    # 4. B√°o c√°o
    print("\n" + "=" * 60)
    print("üìä ACCURACY REPORT (Sampled)")
    print("=" * 60)
    
    if processed_count == 0: return

    precision = total_tp / processed_count
    
    print(f"Triples Evaluated: {processed_count}")
    print("-" * 30)
    print(f"‚úÖ True Positives:  {total_tp}")
    print(f"‚ùå False Positives: {total_fp}")
    print("-" * 30)
    print(f"üéØ PRECISION: {precision:.2%}")
    print("(Note: Recall cannot be calculated accurately without Ground Truth labeling)")
    print("=" * 60)

if __name__ == "__main__":
    main()